{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UDF (USER DEFINE FUNCTION)\n",
    "Spark stores data in dataframes or RDDs—resilient distributed datasets. Think of these like databases. As with a traditional SQL database, e.g. mySQL, you cannot create your own custom function and run that against the database directly. You have to register the function first. That is, save it to the database as if it were one of the built-in database functions, like sum(), average, count(),etc.\n",
    "\n",
    "Three approaches to UDFs\n",
    "There are three ways to create UDFs:\n",
    "\n",
    "df = df.withColumn\n",
    "df = sqlContext.sql(“sql statement from <df>”)\n",
    "\n",
    "    \n",
    "    Below, we create a simple dataframe and RDD. We write a function to convert the only text field in the data structure to an integer. That is something you might do if, for example, you are working with machine learning where all the data must be converted to numbers before you plug that into an algorithm.\n",
    "\n",
    "Notice the imports below. Refer to those in each example, so you know what object to import for each of the three approaches.\n",
    "\n",
    "Below is the complete code for Approach 1. First, we look at key sections. Create a dataframe using the usual approach:\n",
    "\n",
    "# Approach 1: withColumn()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SQLContext\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, FloatType, StringType\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql import Row\n",
    "\n",
    "\n",
    "conf = pyspark.SparkConf() \n",
    "\n",
    " \n",
    "sc = pyspark.SparkContext.getOrCreate(conf=conf)\n",
    "spark = SQLContext(sc)\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"sales\", FloatType(),True),    \n",
    "    StructField(\"employee\", StringType(),True),\n",
    "    StructField(\"ID\", IntegerType(),True)\n",
    "])\n",
    "\n",
    "data = [[ 10.2, \"Fred\",123]]\n",
    "\n",
    "df = spark.createDataFrame(data,schema=schema)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we do two things. First, we create a function colsInt and register it. That registered function calls another function toInt(), which we don’t need to register. The first argument in udf.register(“colsInt”, colsInt) is the name we’ll use to refer to the function. The second is the function we want to register.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+---+----------+\n",
      "|sales|employee| ID| semployee|\n",
      "+-----+--------+---+----------+\n",
      "| 10.2|    Fred|123|1394624364|\n",
      "+-----+--------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "colsInt = udf(lambda z: toInt(z), IntegerType())\n",
    "spark.udf.register(\"colsInt\", colsInt)\n",
    "\n",
    "def toInt(s):\n",
    "    if isinstance(s, str) == True:\n",
    "        st = [str(ord(i)) for i in s]\n",
    "        return(int(''.join(st)))\n",
    "    else:\n",
    "         return Null\n",
    "\n",
    "\n",
    "df2 = df.withColumn( 'semployee',colsInt('employee'))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach 2: Using SQL\n",
    "The first step here is to register the dataframe as a table, so we can run SQL statements against it. df is the dataframe and dftab is the temporary table we create.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.registerDataFrameAsTable(df, \"dftab\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we create a new dataframe df3 from the existing on df and apply the colsInt function to the employee column.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = spark.sql(\"select sales, employee, ID, colsInt(employee) as iemployee from dftab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+---+----------+\n",
      "|sales|employee| ID| iemployee|\n",
      "+-----+--------+---+----------+\n",
      "| 10.2|    Fred|123|1394624364|\n",
      "+-----+--------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
